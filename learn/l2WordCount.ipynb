{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/13 18:52:32 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# import getpass\n",
    "# username = getpass.getuser()\n",
    "# spark = SparkSession. \\\n",
    "# builder. \\\n",
    "# config('spark.ui.port', '0'). \\\n",
    "# config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "# enableHiveSupport(). \\\n",
    "# master('yarn'). \\\n",
    "# getOrCreate()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"sparkWordCount\").getOrCreate()\n",
    "\n",
    "# Path to the text file in HDFS\n",
    "hdfs_path = \"hdfs://localhost:9000/input/input.txt\"\n",
    "# hdfs_path = \"hdfs://localhost:9000/data/recruit_10_14.json\"\n",
    "\n",
    "# Read the text file into a DataFrame (each line is a row)\n",
    "df = spark.read.text(hdfs_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------+\n",
      "|value                                                                                                          |\n",
      "+---------------------------------------------------------------------------------------------------------------+\n",
      "|This is the example text file for word count example also knows as hello world example of the Hadoop ecosystem.|\n",
      "|This example is written for the examples article of java code geek                                             |\n",
      "|The quick brown fox jumps over the lazy dog.                                                                   |\n",
      "|The above line is one of the most famous lines which contains all the english language alphabets.              |\n",
      "+---------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "This is the example text file for word count example also knows as hello world example of the Hadoop ecosystem.\n",
      "This example is written for the examples article of java code geek\n",
      "The quick brown fox jumps over the lazy dog.\n",
      "The above line is one of the most famous lines which contains all the english language alphabets.\n",
      "+---------------------------------------------------------------------------------------------------------------+\n",
      "|value                                                                                                          |\n",
      "+---------------------------------------------------------------------------------------------------------------+\n",
      "|This is the example text file for word count example also knows as hello world example of the Hadoop ecosystem.|\n",
      "|This example is written for the examples article of java code geek                                             |\n",
      "|The quick brown fox jumps over the lazy dog.                                                                   |\n",
      "|The above line is one of the most famous lines which contains all the english language alphabets.              |\n",
      "+---------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)\n",
    "for row in df.collect():\n",
    "    print(row[0])  # Assuming there is only one column\n",
    "# import pandas as pd\n",
    "# pdf = df.toPandas()\n",
    "# print(pdf)\n",
    "# rdd2 = df.flatmap(lambda x: x.split(\" \"))\n",
    "# rdd2.collect()\n",
    "df.select(\"value\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://vtzy-Lenovo-Legion-7-15IMH05.lan:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>HDFSReadTextFileExample</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f5ceab81f10>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'the', 'example', 'text', 'file', 'for', 'word', 'count', 'example', 'also', 'knows', 'as', 'hello', 'world', 'example', 'of', 'the', 'Hadoop', 'ecosystem.', 'This', 'example', 'is', 'written', 'for', 'the', 'examples', 'article', 'of', 'java', 'code', 'geek', 'The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog.', 'The', 'above', 'line', 'is', 'one', 'of', 'the', 'most', 'famous', 'lines', 'which', 'contains', 'all', 'the', 'english', 'language', 'alphabets.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lines = df.rdd.map(lambda row: row[0]).collect()\n",
    "# In từng dòng\n",
    "words = [word for sentence in lines for word in sentence.split()]\n",
    "print(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 1),\n",
       " ('is', 1),\n",
       " ('the', 1),\n",
       " ('example', 1),\n",
       " ('text', 1),\n",
       " ('file', 1),\n",
       " ('for', 1),\n",
       " ('word', 1),\n",
       " ('count', 1),\n",
       " ('example', 1),\n",
       " ('also', 1),\n",
       " ('knows', 1),\n",
       " ('as', 1),\n",
       " ('hello', 1),\n",
       " ('world', 1),\n",
       " ('example', 1),\n",
       " ('of', 1),\n",
       " ('the', 1),\n",
       " ('Hadoop', 1),\n",
       " ('ecosystem.', 1),\n",
       " ('This', 1),\n",
       " ('example', 1),\n",
       " ('is', 1),\n",
       " ('written', 1),\n",
       " ('for', 1),\n",
       " ('the', 1),\n",
       " ('examples', 1),\n",
       " ('article', 1),\n",
       " ('of', 1),\n",
       " ('java', 1),\n",
       " ('code', 1),\n",
       " ('geek', 1),\n",
       " ('The', 1),\n",
       " ('quick', 1),\n",
       " ('brown', 1),\n",
       " ('fox', 1),\n",
       " ('jumps', 1),\n",
       " ('over', 1),\n",
       " ('the', 1),\n",
       " ('lazy', 1),\n",
       " ('dog.', 1),\n",
       " ('The', 1),\n",
       " ('above', 1),\n",
       " ('line', 1),\n",
       " ('is', 1),\n",
       " ('one', 1),\n",
       " ('of', 1),\n",
       " ('the', 1),\n",
       " ('most', 1),\n",
       " ('famous', 1),\n",
       " ('lines', 1),\n",
       " ('which', 1),\n",
       " ('contains', 1),\n",
       " ('all', 1),\n",
       " ('the', 1),\n",
       " ('english', 1),\n",
       " ('language', 1),\n",
       " ('alphabets.', 1)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_rdd = spark.sparkContext.parallelize(words)\n",
    "pair = words_rdd.map(lambda word: (word, 1))\n",
    "pair.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counters = pair.reduceByKey(lambda x,y: x+y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
